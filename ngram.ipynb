{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive ngram approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/alvations/n-gram-language-model-with-nltk - ngram lm  \n",
    "https://www.kaggle.com/anshulrai/cudnnlstm-implementation-93-7-accuracy - amazon reviews loader and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "ngram_size = 6\n",
    "data_amount = 100000000 # number of characters mb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import bz2\n",
    "import gc\n",
    "import chardet\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220626\n"
     ]
    }
   ],
   "source": [
    "train_file = bz2.BZ2File('data/amazonreviews/train.ft.txt.bz2')\n",
    "train_file_lines = train_file.readlines(size=data_amount)\n",
    "\n",
    "del train_file\n",
    "\n",
    "train_file_lines = [x.decode('utf-8') for x in train_file_lines]\n",
    "\n",
    "\n",
    "train_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file_lines]\n",
    "train_sentences = [x.split(' ', 1)[1][:-1].lower() for x in train_file_lines]\n",
    "\n",
    "for i in range(len(train_sentences)):\n",
    "    train_sentences[i] = re.sub('\\d','0',train_sentences[i])\n",
    " \n",
    "                                                      \n",
    "for i in range(len(train_sentences)):\n",
    "    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n",
    "        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])\n",
    "        \n",
    "\n",
    "del train_file_lines\n",
    "\n",
    "gc.collect()\n",
    "print(len(train_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit. Predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lmura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import pad_sequence\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.lm.preprocessing import flatten\n",
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f0af721a8e140ba8dc00f3fba2db0c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=220626.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "# This is slow, I know, wygd\n",
    "tokenized_sentences = list(tqdm(map(word_tokenize, train_sentences), total=len(train_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "training_ngrams, padded_sentences = padded_everygram_pipeline(ngram_size, tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db16fd0ab4b446ba883eb0751ab2ada6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=220626.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-e0fdfd5b1d08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMLE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngram_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_ngrams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_sentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadded_sentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\torch-env\\lib\\site-packages\\nltk\\lm\\api.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, text, vocabulary_text)\u001b[0m\n\u001b[0;32m    132\u001b[0m                 )\n\u001b[0;32m    133\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcounts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\torch-env\\lib\\site-packages\\nltk\\lm\\counter.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, ngram_text)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mngram\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mngram_order\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\torch-env\\lib\\collections\\__init__.py\u001b[0m in \u001b[0;36m__missing__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    552\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 554\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0m__missing__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    555\u001b[0m         \u001b[1;34m'The count of elements not in the Counter is zero.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m         \u001b[1;31m# Needed so that self[missing_item] does not raise KeyError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from nltk.lm import MLE\n",
    "model = MLE(ngram_size)\n",
    "model.fit(tqdm(training_ngrams, total=len(train_sentences)), padded_sentences)\n",
    "print(model.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this iron is good . yet when i picked up this book . i strongly suggest that you read them all . these books allowed my\n"
     ]
    }
   ],
   "source": [
    "def make_continuation(text):\n",
    "    seed = word_tokenize(text)\n",
    "    output = ' '.join(seed + model.generate(20, text_seed=seed))\n",
    "    output = output[:output.find('</s>')] if output.find('</s>') != -1 else output\n",
    "    return output\n",
    "\n",
    "print(make_continuation('this iron is good . yet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draws various pathes model could have gone. Also saves this tree to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"562.5px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 15px;\" version=\"1.1\" viewBox=\"0,0,796.5000000000003,562.5\" width=\"796.5px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">this</text></svg><svg width=\"100%\" x=\"0%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">iron</text></svg><svg width=\"100%\" x=\"0%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">is</text></svg><svg width=\"100%\" x=\"0%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">good</text></svg><svg width=\"100%\" x=\"0%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">yet</text></svg><svg width=\"100%\" x=\"0%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">the</text></svg><svg width=\"100%\" x=\"0%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">basket</text></svg><svg width=\"100%\" x=\"0%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">is</text></svg><svg width=\"100%\" x=\"0%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">still</text></svg><svg width=\"100%\" x=\"0%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">in</text></svg><svg width=\"100%\" x=\"0%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">excellent</text></svg><svg width=\"100%\" x=\"0%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">shape</text></svg><svg width=\"100%\" x=\"0%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">.</text></svg><svg width=\"97.9284%\" x=\"0%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">i</text></svg><svg width=\"92.8846%\" x=\"0%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">bought</text></svg><svg width=\"100%\" x=\"0%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">it</text></svg><svg width=\"100%\" x=\"0%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">used</text></svg><svg width=\"93.5818%\" x=\"0%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">and</text></svg><svg width=\"81.1947%\" x=\"0%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">was</text></svg><svg width=\"100%\" x=\"0%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">very</text></svg><svg width=\"100%\" x=\"0%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">pleased</text></svg><svg width=\"100%\" x=\"0%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">with</text></svg><svg width=\"91.2807%\" x=\"0%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">the</text></svg><svg width=\"13.7313%\" x=\"0%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">efficient</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"6.86567%\" y1=\"1.2em\" y2=\"1.5em\" /><svg width=\"7.76119%\" x=\"13.7313%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">0-day</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"17.6119%\" y1=\"1.2em\" y2=\"1.5em\" /><svg width=\"12.2388%\" x=\"21.4925%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">pan..its</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"27.6119%\" y1=\"1.2em\" y2=\"1.5em\" /><svg width=\"7.76119%\" x=\"33.7313%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">price</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"37.6119%\" y1=\"1.2em\" y2=\"1.5em\" /><svg width=\"12.2388%\" x=\"41.4925%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">purchase</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"47.6119%\" y1=\"1.2em\" y2=\"1.5em\" /><svg width=\"7.76119%\" x=\"53.7313%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">speed</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"57.6119%\" y1=\"1.2em\" y2=\"1.5em\" /><svg width=\"10.7463%\" x=\"61.4925%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">quality</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"66.8657%\" y1=\"1.2em\" y2=\"1.5em\" /><svg width=\"6.26866%\" x=\"72.2388%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">book</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"75.3731%\" y1=\"1.2em\" y2=\"1.5em\" /><svg width=\"10.7463%\" x=\"78.5075%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">picture</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"83.8806%\" y1=\"1.2em\" y2=\"1.5em\" /><svg width=\"10.7463%\" x=\"89.2537%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">service</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"94.6269%\" y1=\"1.2em\" y2=\"1.5em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"45.6403%\" y1=\"1.2em\" y2=\"1.5em\" /><svg width=\"2.99728%\" x=\"91.2807%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">it</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"92.7793%\" y1=\"1.2em\" y2=\"1.5em\" /><svg width=\"5.72207%\" x=\"94.2779%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">them</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"97.139%\" y1=\"1.2em\" y2=\"1.5em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"1.5em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"1.5em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"1.5em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"40.5973%\" y1=\"1.2em\" y2=\"1.5em\" /><svg width=\"2.43363%\" x=\"81.1947%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">it</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"82.4115%\" y1=\"1.2em\" y2=\"1.5em\" /><svg width=\"5.75221%\" x=\"83.6283%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">cheap</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"86.5044%\" y1=\"1.2em\" y2=\"1.5em\" /><svg width=\"3.53982%\" x=\"89.3805%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">the</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"91.1504%\" y1=\"1.2em\" y2=\"1.5em\" /><svg width=\"3.53982%\" x=\"92.9204%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">not</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"94.6903%\" y1=\"1.2em\" y2=\"1.5em\" /><svg width=\"3.53982%\" x=\"96.4602%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">did</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"98.2301%\" y1=\"1.2em\" y2=\"1.5em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"46.7909%\" y1=\"1.2em\" y2=\"1.5em\" /><svg width=\"4.34783%\" x=\"93.5818%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">from</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"95.7557%\" y1=\"1.2em\" y2=\"1.5em\" /><svg width=\"2.07039%\" x=\"97.9296%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"98.9648%\" y1=\"1.2em\" y2=\"1.5em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"1.5em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"1.5em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"46.4423%\" y1=\"1.2em\" y2=\"1.5em\" /><svg width=\"5%\" x=\"92.8846%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">would</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"95.3846%\" y1=\"1.2em\" y2=\"1.5em\" /><svg width=\"2.11538%\" x=\"97.8846%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">do</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"98.9423%\" y1=\"1.2em\" y2=\"1.5em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"48.9642%\" y1=\"1.2em\" y2=\"1.5em\" /><svg width=\"2.07156%\" x=\"97.9284%\" y=\"1.5em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">it</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"98.9642%\" y1=\"1.2em\" y2=\"1.5em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"1.5em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"1.5em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"1.5em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"1.5em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"1.5em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"1.5em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"1.5em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"1.5em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"1.5em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"1.5em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"1.5em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"1.5em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"1.5em\" /></svg>"
      ],
      "text/plain": [
       "TreeLayout(['this', ['iron', ['is', ['good', ['.', ['yet', ['the', ['basket', ['is', ['still', ['in', ['excellent', ['shape', ['.', ['i', ['bought', ['it', ['used', ['and', ['was', ['very', ['pleased', ['with', ['the', 'efficient', '0-day', 'pan..its', 'price', 'purchase', 'speed', 'quality', 'book', 'picture', 'service'], 'it', 'them']]]], 'it', 'cheap', 'the', 'not', 'did'], 'from', '.']]], 'would', 'do'], 'it']]]]]]]]]]]]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this iron is good . yet the basket is still in excellent shape . i bought it used and was very pleased with the service and\n"
     ]
    }
   ],
   "source": [
    "import svgling\n",
    "import random\n",
    "\n",
    "def draw_word_tree(model, text):\n",
    "    words = ['*'] * (ngram_size - 2) + word_tokenize(text)\n",
    "\n",
    "    prev_node = [words[len(words) - ngram_size - 2 + ngram_size - 1]] + \\\n",
    "                list(model.counts[words[len(words) - \\\n",
    "                ngram_size - 1:len(words) - ngram_size - 2 + ngram_size ]].keys())[:10]\n",
    "    for i in range(len(words) - ngram_size - 1)[::-1]:\n",
    "        predictions = list(model.counts[words[i:i + ngram_size - 1]].keys())[:10]\n",
    "        prev_word = prev_node[0]\n",
    "        leaves = [prev_word] + list(set(predictions) - set([prev_word]))\n",
    "        new_node = [words[i + ngram_size - 2]] + leaves\n",
    "        index = new_node.index(prev_word)\n",
    "        new_node[index] = prev_node\n",
    "        prev_node = new_node\n",
    "        \n",
    "    svgling.draw_tree(prev_node, distance_to_daughter=0.5, leaf_padding=0.2, font_size=15, horiz_spacing=svgling.core.HorizSpacing.TEXT).get_svg().saveas(\"test.svg\")\n",
    "    display(svgling.draw_tree(prev_node, distance_to_daughter=0.5, leaf_padding=0.2, font_size=15, horiz_spacing=svgling.core.HorizSpacing.TEXT))\n",
    "    \n",
    "output = make_continuation('this iron is good . yet')\n",
    "draw_word_tree(model, output)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pickle saving/restoring is impossibly slow. Training is faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open('6gram_small.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('6gram_small.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose best from sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import KeyedVectors\n",
    "# word2vec = KeyedVectors.load_word2vec_format(\n",
    "#     'GoogleNews-vectors-negative300.bin.gz', \n",
    "#     binary=True)\n",
    "\n",
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "vec_king = wv['king']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "torch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
